# ğŸ¤– TARA â€“ Memo-Local Voice Assistant

TARA is a fully offline, privacy-first voice assistant that listens, thinks, speaks, and remembers â€” without relying on cloud services. It uses **Whisper.cpp** for speech-to-text, **LLaMA3** (via Ollama) for intelligence, and **Piper TTS** for voice output.

---

## ğŸ“Œ Features

- ğŸ™ï¸ **Voice-to-Text (STT)**: High-performance local transcription via Whisper.cpp.
- ğŸ¤– **Local Intelligence**: Powered by LLaMA3:8B using Ollama.
- ğŸ§  **Persistent Memory**: Remembers your conversations in a local `memory.json` file.
- ğŸ”Š **Text-to-Speech (TTS)**: High-quality, low-latency speech using Piper (running in Docker).
- ğŸŒ **Flask API**: Web interface for interaction and memory management.
- ğŸ” **100% Offline**: No data leaves your machine.
  
---

## ğŸ› ï¸ Technologies Used

| Component            |        Tool / Framework             |
|----------------------|-------------------------------------|
| **Audio Recording**  | `sounddevice`, `wave` (Python)      |
| **Transcription**    | `whisper.cpp` + `ggml-base.en.bin`  |
| **LLM Inference**    | `LLaMA3` via `Ollama` (localhost API)|
| **Backend**          | `Flask`, `asyncio`, `json`          |
| **TTS**              | `Piper` (Docker) + Wyoming Protocol |
| **Memory Storage**   | Local `memory.json` file            |
| **API Testing**      | Postman, Browser                    |

---

## ğŸ§© Architecture Overview

![archtara1](https://github.com/user-attachments/assets/27b45a43-6838-40f1-9dd0-1ecdc252e5cf)

## WorkFlow

1. ğŸ§ **`record_audio.py`** captures user audio as `output.wav`
2. ğŸ§  **`whisper.cpp`** transcribes the audio â†’ `output.txt`
3. ğŸ§© **`assistant.py`** processes the text:
   - If "clear memory" â†’ clears `memory.json`
   - Else â†’ sends context + prompt to LLaMA3 via Ollama
4. ğŸ§  Response is saved in memory (`memory.json`)
5. ğŸ”Š Reply is spoken via Piper TTS and played back
6. ğŸŒ Optional: Interact via REST API

---

## ğŸŒ Flask API Endpoints

| Method | Route         | Description                                   |
|--------|---------------|---------------------------------------------- |
| POST   | `/transcribe` | Runs full pipeline (record â†’ respond â†’ speak) |
| POST   | `/ask`        | Accepts direct text input, responds & speaks  |
| GET    | `/memory`     | Returns recent conversation history           |

---

## ğŸ› ï¸ Prerequisites

Before you begin, ensure you have the following installed:

1.  **Python 3.9+**: [Download Python](https://www.python.org/downloads/)
2.  **Docker Desktop**: [Download Docker](https://www.docker.com/products/docker-desktop/) (Required for Piper TTS)
3.  **Git**: [Download Git](https://git-scm.com/downloads)
4.  **Ollama**: [Download Ollama](https://ollama.com/) (For running LLaMA3)
5.  **C++ Build Tools**: (Required for building Whisper.cpp)
    *   *Windows*: Install Visual Studio 2019/2022 Community with "Desktop development with C++".

---

## ğŸš€ Installation & Setup

### 1. Clone the Repository

Open your terminal (PowerShell or Command Prompt) and run:

```bash
git clone https://github.com/your-username/tara-voice-assistant.git
cd tara-voice-assistant
```

### 2. Set Up Python Environment

Create a virtual environment to keep dependencies isolated:

```bash
python -m venv venv
```

Activate the virtual environment:
*   **Windows**: `.\venv\Scripts\activate`
*   **Mac/Linux**: `source venv/bin/activate`

### 3. Install Python Dependencies

```bash
pip install -r backend/requirements.txt
```

### 4. Build Whisper.cpp (Speech-to-Text)

You need to compile the Whisper engine locally.

1.  Navigate to the backend directory:
    ```bash
    cd backend
    ```
2.  Clone and build `whisper.cpp`:
    ```bash
    git clone https://github.com/ggerganov/whisper.cpp
    cd whisper.cpp
    cmake -B build
    cmake --build build --config Release
    ```
    *(Note: This creates the `whisper-cli.exe` executable inside `build/bin/Release/`).*

### 5. Download the Whisper Model

While inside the `whisper.cpp` directory, download the base English model:

*   **Windows (PowerShell)**:
    ```powershell
    # Download the model downloader script if not present, then run it
    python models/download-ggml-model.py base.en
    ```
    *Alternatively, manually download `ggml-base.en.bin` from [Hugging Face](https://huggingface.co/ggerganov/whisper.cpp) and place it in `backend/models/` folder.*

    **Important**: Ensure `ggml-base.en.bin` is moved to the `backend/models/` folder so the application can find it.
    ```bash
    # Move model to correct location (adjust path as needed)
    move models/ggml-base.en.bin ../models/
    ```

### 6. Set Up Piper TTS (Text-to-Speech)

We use Docker to run the Piper TTS server. This is the easiest and most reliable method.

Run the following command in your terminal:

```bash
docker run -it --rm -v "$HOME/piper/voices:/voices" -p 10200:10200 rhasspy/wyoming-piper --voice en/en_US-lessac-medium
```

*   **What this does**:
    *   Downloads the `rhasspy/wyoming-piper` image.
    *   Starts a server on port `10200`.
    *   Uses the `en_US-lessac-medium` voice.

### 7. Set Up Ollama (LLaMA3 Model)

1.  Ensure Ollama is running (check your system tray or run `ollama serve`).
2.  Pull the LLaMA3 model:
    ```bash
    ollama run llama3:8b
    ```
    *(After the model downloads and you see a prompt `>>>`, you can type `/bye` to exit the chat, but keep the Ollama background service running).*

---

## â–¶ï¸ Running the Assistant

Once everything is set up:

1.  **Start the Backend**:
    Make sure your virtual environment is activated (`.\venv\Scripts\activate`).
    ```bash
    cd backend
    python assistant.py
    ```

    *The assistant will now be listening. Speak into your microphone!*

2.  **Start the Flask API (Optional)**:
    If you want to use the web interface or API endpoints:
    ```bash
    python app.py
    ```

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ backend/                  # Python backend
â”‚   â”œâ”€â”€ app.py                # Flask API
â”‚   â”œâ”€â”€ assistant.py          # Main logic (STT -> LLM -> TTS)
â”‚   â”œâ”€â”€ record_audio.py       # Audio recorder script
â”‚   â”œâ”€â”€ my_tts.py             # Client for Piper TTS (Wyoming protocol)
â”‚   â”œâ”€â”€ llm.py                # Interface for Ollama (LLaMA3)
â”‚   â”œâ”€â”€ memory.json           # Conversation memory
â”‚   â”œâ”€â”€ models/               # Directory for Whisper binary models
â”‚   â””â”€â”€ whisper.cpp/          # Source code for Whisper engine
â”œâ”€â”€ frontend/                 # React frontend
â”‚   â”œâ”€â”€ src/
â”‚   â””â”€â”€ public/
â””â”€â”€ README.md
```

## ğŸ¤ Contributions
Contributions are welcome!
1. Fork the repository.
2. Create a feature branch.
3. Commit your changes.
4. Submit a pull request.

## Author

**Yeshwanth Goud**

*Data Scientist | Full Stack & ML Enthusiast*
